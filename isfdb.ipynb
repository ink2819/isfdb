{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get author/artist page links from data and scrape each to get title links\n",
    "authors = pd.read_csv('sisters_of_tomorrow_names.csv')\n",
    "links = authors['link']\n",
    "work_title_link = []\n",
    "\n",
    "for i in links:\n",
    "    site = requests.get (i)\n",
    "    html_source = site.content\n",
    "    soup = BeautifulSoup(html_source)\n",
    "    a_tags = soup.find_all('a', class_=\"italic\")\n",
    "    for a in a_tags:\n",
    "        work_title_link.append(a['href']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform title links to cover page links that include all documented image covers for each title\n",
    "#i do this because each title has a list of publications(different editions, etc.) but not all publications have cover images. The titlecovers page lists all of the covers for a given title.\n",
    "coverpages = []\n",
    "work_title_link = pd.read_csv('Title_links.csv')['Links']\n",
    "for i in work_title_link: \n",
    "    coverpages.append(i.replace(\"https://www.isfdb.org/cgi-bin/title.cgi?\", \"https://www.isfdb.org/cgi-bin/titlecovers.cgi?\"))\n",
    "\n",
    "#get internal ids for each publication listed on the titlecover page.\n",
    "\n",
    "publication = []\n",
    "image_link=[] #also logging image link and title for debugging\n",
    "work=[]\n",
    "\n",
    "for i in coverpages[0:200]: \n",
    "    site = requests.get (i)\n",
    "    html_source = site.content\n",
    "    soup = BeautifulSoup(html_source)\n",
    "    a_tags = soup.find_all('a', dir=\"ltr\")\n",
    "    \n",
    "    for a in a_tags:\n",
    "        image= a.find('img')\n",
    "        if image:\n",
    "            image_link.append(image['src'])  \n",
    "            publication.append(a['href']) \n",
    "\n",
    "    worklink = soup.find('a', class_=\"bold\")\n",
    "    if worklink:\n",
    "        work.append(worklink['href']) \n",
    "\n",
    "covers = []\n",
    "covers.append({'image_link': image_link, 'publication_link': publication , 'work_link': work})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape all publications info by internal id for each publication \n",
    "id = []\n",
    "for i in publication: \n",
    "    id.append(i.split(\"?\", 1)[-1])\n",
    "\n",
    "title = []\n",
    "record = []\n",
    "coverimage_link = []\n",
    "author = []\n",
    "publisher = []\n",
    "date = []\n",
    "price = []\n",
    "pages = []\n",
    "binding = []\n",
    "tag = []\n",
    "publication_type = []\n",
    "note = []\n",
    "\n",
    "#BS4 to parse the xml from ISFDB api links \n",
    "for i in id:\n",
    "    site = requests.get ('https://www.isfdb.org/cgi-bin/rest/getpub_by_internal_ID.cgi?'+ i)\n",
    "    source = site.content\n",
    "    soup = BeautifulSoup(source, 'xml')\n",
    "    pub = soup.find('Publication')\n",
    "    record.append(pub.find('Record').text if pub.find('Record') else None)\n",
    "    title.append(pub.find('Title').text if pub.find('Title') else None)\n",
    "    coverimage_link.append(pub.find('Image').text if pub.find('Image') else None)\n",
    "    author.append(pub.find('Author').text if pub.find('Author') else None)\n",
    "    publisher.append(pub.find('Publisher').text if pub.find('Publisher') else None)\n",
    "    date.append(pub.find('Year').text if pub.find('Year') else None)\n",
    "    price.append(pub.find('Price').text if pub.find('Price') else None)\n",
    "    pages.append(pub.find('Pages').text if pub.find('Pages') else None)\n",
    "    binding.append(pub.find('Binding').text if pub.find('Binding') else None)\n",
    "    tag.append(pub.find('Tag').text if pub.find('Tag') else None)\n",
    "    publication_type.append(pub.find('Type').text if pub.find('Type') else None)\n",
    "    note.append(pub.find('Note').text if pub.find('Note') else None)\n",
    "\n",
    "All_SF = pd.DataFrame({\n",
    "    \"Record\": record,\n",
    "    \"Title\": title,\n",
    "    \"Cover Image Link\": coverimage_link,\n",
    "    \"Author\": author,\n",
    "    \"Publisher\": publisher,\n",
    "    \"Date\": date,\n",
    "    \"Price\": price,\n",
    "    \"Pages\": pages,\n",
    "    \"Binding\": binding,\n",
    "    \"Tag\": tag,\n",
    "    \"Publication Type\": publication_type,\n",
    "    \"Note\": note\n",
    "})\n",
    "\n",
    "All_SF.to_csv('pulp_publications1.csv', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run google vit model for all the images\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"image-classification\", model=\"google/vit-base-patch16-224\")\n",
    "\n",
    "pulps = pd.read_csv('pulp_publications1.csv')\n",
    "pulps = pulps[pulps['Binding'] == \"pulp\"]#filter for just the pulp publications\n",
    "images = pulps['Cover Image Link']\n",
    "\n",
    "labels_google = []\n",
    "\n",
    "for i in images:\n",
    "    output = pipe(i)\n",
    "    labels_google.append(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run microsoft model for all the images\n",
    "pipe = pipeline(\"image-classification\", model=\"microsoft/resnet-50\")\n",
    "labels_microsoft = []\n",
    "\n",
    "for i in images:\n",
    "    output = pipe(i)\n",
    "    labels_microsoft.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run oschamp/vit-artworkclassifier model for all the images\n",
    "\n",
    "pipe = pipeline(\"image-classification\", model=\"oschamp/vit-artworkclassifier\")\n",
    "labels_oschamp = []\n",
    "\n",
    "for i in images:\n",
    "    output = pipe(i)\n",
    "    labels_oschamp.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append generated labels to the csv, keeping only the labels with a confidance score above 0.3\n",
    "\n",
    "def extract_labels(data): #tried writing a function here to be reused for each model's output.\n",
    "    return [\n",
    "        ', '.join([item['label'] for item in output if item['score'] >= 0.3])\n",
    "        for output in data\n",
    "    ]\n",
    "\n",
    "#append them to the working dataframe\n",
    "google_vit_tags = extract_labels(labels_google)\n",
    "pulps['google_vit'] = google_vit_tags\n",
    "\n",
    "microsoft_tags = extract_labels(labels_microsoft)\n",
    "pulps['microsoft'] = microsoft_tags\n",
    "\n",
    "oschamp_tags = extract_labels(labels_oschamp)\n",
    "pulps['oschamp'] = oschamp_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load tagging results from csv\n",
    "outputs = pd.read_csv(\"outputs.csv\")\n",
    "\n",
    "#extract the record id from image name\n",
    "outputs['Record'] = outputs['image name'].str.split('.').str[0]\n",
    "\n",
    "#set an empty column in pulps and add tags from the wd14 tagging results by matching record id\n",
    "pulps['wd14'] = \"\"\n",
    "for i in range(len(pulps)):\n",
    "    for j in range(len(outputs)):\n",
    "        if pulps.loc[i, 'Record'] == outputs.loc[j, 'Record']:\n",
    "            pulps.loc[i, 'wd14'] = outputs.loc[j, 'taglist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pulps.to_csv('pulp_publications2.csv', index=True, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
